{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DowMtZF0B6UE"
      },
      "outputs": [],
      "source": [
        "# @title [1] Setup: Install Libraries\n",
        "\n",
        "!pip install openai pandas scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [2] Configuration: API Key, Model, and File Paths\n",
        "# In this cell, we configure all the essential parameters for our experiment.\n",
        "# You must replace \"<YOUR_OPENROUTER_API_KEY>\" with your actual key.\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "\n",
        "# --- REQUIRED: SET YOUR API KEY HERE ---\n",
        "# You can get a key from https://openrouter.ai/keys\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "\n",
        "# --- MODEL CONFIGURATION ---\n",
        "# You can change this to any model available on OpenRouter.\n",
        "# Find model names here: https://openrouter.ai/models\n",
        "# Example: \"anthropic/claude-3.5-sonnet\", \"google/gemini-pro-1.5\", \"mistralai/mistral-large\"\n",
        "MODEL_TO_TEST = \"deepseek-reasoner\"\n",
        "\n",
        "# --- DATASET CONFIGURATION ---\n",
        "# Assumes 'PREMOVE.csv' is uploaded to the Colab session's root directory.\n",
        "DATASET_PATH = \"PREMOVE.csv\"\n",
        "\n",
        "# --- EXPERIMENT CONFIGURATION ---\n",
        "# Define the prompting strategies you want to test. The number indicates the number of few-shot examples.\n",
        "SHOT_STRATEGIES = {\n",
        "    \"zero_shot\": 0,\n",
        "    \"one_shot\": 1,\n",
        "    \"two_shot\": 2,\n",
        "    \"five_shot\": 5\n",
        "}\n",
        "# To run a quick test, you can limit the number of rows processed from the dataset.\n",
        "# Set to None to process all rows.\n",
        "MAX_ROWS_TO_PROCESS = None # For demonstration. Set to None for the full run.\n",
        "\n",
        "\n",
        "# --- Initialize the OpenAI Client for OpenRouter ---\n",
        "# We point the base_url to OpenRouter's API endpoint.\n",
        "client = openai.OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=OPENROUTER_API_KEY,\n",
        ")\n",
        "\n",
        "print(\"Configuration loaded successfully.\")"
      ],
      "metadata": {
        "id": "O-b2trK6CG7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [3] Data Loading and Preparation\n",
        "# This cell loads the PREMOVE.csv file into a pandas DataFrame,\n",
        "# selects the necessary columns, and cleans it up for use.\n",
        "\n",
        "try:\n",
        "    # Load the dataset from the specified path\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "    # Define the columns we need for this task\n",
        "    required_columns = ['VERB TOKEN', 'SENTENCE', 'PREVERB', 'PREVERB SEMANTICS']\n",
        "\n",
        "    # Keep only the required columns\n",
        "    df = df[required_columns]\n",
        "\n",
        "    # Rename columns to be more Python-friendly (no spaces)\n",
        "    df.rename(columns={\n",
        "        'VERB TOKEN': 'verb_token',\n",
        "        'SENTENCE': 'sentence',\n",
        "        'PREVERB': 'preverb',\n",
        "        'PREVERB SEMANTICS': 'ground_truth_semantics'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Drop any rows where essential data might be missing\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Reset the index after dropping rows\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Limit the number of rows if specified for testing\n",
        "    if MAX_ROWS_TO_PROCESS is not None:\n",
        "        df = df.head(MAX_ROWS_TO_PROCESS)\n",
        "        print(f\"--- Using a limited dataset of {MAX_ROWS_TO_PROCESS} rows for this run. ---\")\n",
        "\n",
        "\n",
        "    print(\"Dataset loaded and prepared successfully.\")\n",
        "    print(f\"Total rows to process: {len(df)}\")\n",
        "    print(\"\\nFirst 5 rows of the prepared data:\")\n",
        "    #display(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file '{DATASET_PATH}' was not found.\")\n",
        "    print(\"Please make sure you have uploaded the CSV file to your Colab environment.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")"
      ],
      "metadata": {
        "id": "y1N3h6CkCKmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [4] Core Functions: Prompting and API Interaction\n",
        "# This cell contains the functions that form the backbone of our experiment.\n",
        "\n",
        "def create_prompt(current_row, all_data, num_shots):\n",
        "    '''\n",
        "    Creates a prompt by placing examples in the system message.\n",
        "\n",
        "    Args:\n",
        "        current_row (pd.Series): The row containing the question to ask.\n",
        "        all_data (pd.DataFrame): The entire dataset to sample from for few-shot examples.\n",
        "        num_shots (int): The number of examples to include (0 for zero-shot).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of message dictionaries for the API call.\n",
        "    '''\n",
        "    # --- System Prompt Construction ---\n",
        "    system_prompt_content = (\"\"\"# Role and Goal\n",
        "\n",
        "You are an expert in Ancient Greek and Latin Linguistics. You will be given an Ancient Greek or Latin sentence that includes a preverbed motion verb. Your primary task is to identify the semantics of the preverb, considering the compositionality of the verb.\n",
        "\n",
        "# Guidelines\n",
        "\n",
        "*   The semantics of the preverb must be annotated depending on the specific meaning that the preverb acquires in context.\n",
        "*   If a preverb does not possess a clear lexical meaning and only contributes to telicising the verbal base, tag it with â€˜completelyâ€™.\n",
        "*   In some occurrences, two meanings may overlap. Annotate both.\n",
        "\n",
        "# Output Requirements\n",
        "\n",
        "*   Respond with ONLY one or two English meanings from the list provided below. These can be adverbs, prepositions, or multi-word expressions.\n",
        "*   Provide nothing else â€” no explanations, no conversational text, and no punctuation beyond what is in the expression itself. If you need to annotate two meanings, separate them with a comma followed by a whitespace.\n",
        "\n",
        "# Allowed Meanings\n",
        "\n",
        "*   about\n",
        "*   across\n",
        "*   after\n",
        "*   again\n",
        "*   against\n",
        "*   along\n",
        "*   among\n",
        "*   around\n",
        "*   away\n",
        "*   back\n",
        "*   before\n",
        "*   behind\n",
        "*   between\n",
        "*   beyond\n",
        "*   completely\n",
        "*   distributively\n",
        "*   down\n",
        "*   downstream\n",
        "*   downwards\n",
        "*   forth\n",
        "*   forward\n",
        "*   from\n",
        "*   from the coast to the inland\n",
        "*   from the coast to the sea\n",
        "*   from the inland to the coast\n",
        "*   from the sea to the coast\n",
        "*   from under\n",
        "*   in\n",
        "*   into\n",
        "*   on\n",
        "*   onto\n",
        "*   onwards\n",
        "*   out\n",
        "*   out of\n",
        "*   over\n",
        "*   past\n",
        "*   through\n",
        "*   to\n",
        "*   together\n",
        "*   together with\n",
        "*   towards\n",
        "*   under\n",
        "*   up\n",
        "*   upon\n",
        "*   upstream\n",
        "*   upwards\n",
        "*   (idea of change)\n",
        "*   (idea of destruction/death)\n",
        "*   (idea of participation)\n",
        "*   (malefactive)\n",
        "*   (none)\"\"\")\n",
        "\n",
        "    if num_shots > 0:\n",
        "        # Exclude the current row from the sampling pool\n",
        "        examples_pool = all_data.drop(current_row.name)\n",
        "        examples = examples_pool.sample(n=num_shots)\n",
        "\n",
        "        example_texts = []\n",
        "        for _, ex in examples.iterrows():\n",
        "            example_texts.append(\n",
        "                f\"Verb: {ex['verb_token']}\\n\"\n",
        "                f\"Sentence: \\\"{ex['sentence']}\\\"\\n\"\n",
        "                f\"Preverb: {ex['preverb']}\\n\"\n",
        "                f\"Meaning: {ex['ground_truth_semantics']}\"\n",
        "            )\n",
        "\n",
        "        system_prompt_content += \"\\n\\nHere are some examples of the task:\\n\\n---\\n\"\n",
        "        system_prompt_content += \"\\n---\\n\".join(example_texts)\n",
        "        system_prompt_content += \"\\n---\\n\\nNow, perform the same analysis for the following case.\"\n",
        "\n",
        "    # --- User Prompt Construction ---\n",
        "    user_prompt_content = (\n",
        "        f\"Verb: {current_row['verb_token']}\\n\"\n",
        "        f\"Sentence: \\\"{current_row['sentence']}\\\"\\n\"\n",
        "        f\"Preverb: {current_row['preverb']}\\n\"\n",
        "        f\"Meaning:\"\n",
        "    )\n",
        "\n",
        "    # --- Final Message Assembly ---\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "\n",
        "def get_llm_response(messages, model):\n",
        "    \"\"\"\n",
        "    Sends a request to the OpenRouter API and returns the model's response.\n",
        "\n",
        "    Args:\n",
        "        messages (list): The list of message dictionaries for the prompt.\n",
        "        model (str): The name of the model to query.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the model's response, or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0.3, # Set to 0 for deterministic, factual outputs\n",
        "            max_tokens=None,   # The answer should be short\n",
        "        )\n",
        "        response_text = completion.choices[0].message.content\n",
        "        # Clean the output: remove extra whitespace and quotes\n",
        "        return response_text.strip().replace('\"', '').replace(\"'\", \"\")\n",
        "    except Exception as e:\n",
        "        print(f\"  !! API Error: {e}\")\n",
        "        return \"API_ERROR\"\n",
        "\n",
        "print(\"Core functions updated and defined.\")"
      ],
      "metadata": {
        "id": "jL-Cw2naCO3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [5] Evaluation Engine: Run Experiments\n",
        "# This is the main part of the notebook. It iterates through each prompting strategy,\n",
        "# queries the model for every row, and evaluates the results.\n",
        "\n",
        "all_results_dfs = {}\n",
        "\n",
        "for strategy_name, num_shots in SHOT_STRATEGIES.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ðŸš€ Starting evaluation for: {strategy_name.upper()} ({num_shots}-shot)\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    results_data = []\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        print(f\"  -> Processing row {index + 1}/{len(df)}...\")\n",
        "\n",
        "        # 1. Create the prompt using our unified function\n",
        "        if num_shots > 0 and len(df) <= num_shots:\n",
        "            print(f\"  !! Warning: Not enough data ({len(df)} rows) to create a {num_shots}-shot prompt. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        prompt_messages = create_prompt(row, df, num_shots)\n",
        "\n",
        "        # 2. Get the LLM's prediction\n",
        "        prediction = get_llm_response(prompt_messages, MODEL_TO_TEST)\n",
        "\n",
        "        # Add a small delay to respect rate limits, if any\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # 3. Normalize ground truth and prediction for accurate comparison\n",
        "        ground_truth = str(row['ground_truth_semantics']).lower().strip()\n",
        "        prediction_normalized = prediction.lower().strip()\n",
        "\n",
        "        # 4. Check if the prediction is correct (using the raw, unquoted prediction)\n",
        "        is_correct = (ground_truth == prediction_normalized)\n",
        "\n",
        "        # 5. Store the results (now including the verb_token)\n",
        "        results_data.append({\n",
        "            'verb_token': row['verb_token'],\n",
        "            'sentence': row['sentence'],\n",
        "            'preverb': row['preverb'],\n",
        "            'ground_truth': row['ground_truth_semantics'],\n",
        "            'llm_prediction': f'\"{prediction}\"', # Enclose in quotes for CSV integrity\n",
        "            'is_correct': is_correct\n",
        "        })\n",
        "\n",
        "    # --- Analysis and Reporting for the current strategy ---\n",
        "    if not results_data:\n",
        "        print(\"\\nNo results were generated for this strategy.\")\n",
        "        continue\n",
        "\n",
        "    # Convert results to a DataFrame\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    all_results_dfs[strategy_name] = results_df\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = results_df['is_correct'].mean()\n",
        "\n",
        "    print(f\"\\n--- ðŸ“Š Results for {strategy_name.upper()} ---\")\n",
        "    print(f\"Model Tested: {MODEL_TO_TEST}\")\n",
        "    print(f\"Total Items Evaluated: {len(results_df)}\")\n",
        "    print(f\"Correct Predictions: {results_df['is_correct'].sum()}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(\"------------------------------------------\\n\")\n",
        "\n",
        "    # Save results to a CSV file. The quotes are now part of the data.\n",
        "    output_filename = f\"results_{MODEL_TO_TEST.replace('/', '_')}_{strategy_name}.csv\"\n",
        "    results_df.to_csv(output_filename, index=False)\n",
        "    print(f\"âœ… Detailed results saved to '{output_filename}'\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All evaluations complete!\")"
      ],
      "metadata": {
        "id": "yTiU1K0jCTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [6] Download Result Files\n",
        "# This cell provides the code to download the generated CSV files to your local machine.\n",
        "\n",
        "if all_results_dfs:\n",
        "    print(\"Preparing result files for download...\")\n",
        "    for strategy_name in all_results_dfs.keys():\n",
        "        filename = f\"results_{MODEL_TO_TEST.replace('/', '_')}_{strategy_name}.csv\"\n",
        "        print(f\"  - Downloading {filename}...\")\n",
        "        files.download(filename)\n",
        "else:\n",
        "    print(\"No result files were generated to download.\")"
      ],
      "metadata": {
        "id": "V_AyOvsLCXoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}