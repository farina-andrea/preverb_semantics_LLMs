{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6360,
     "status": "ok",
     "timestamp": 1753260744107,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "Rhmw0zGzcDbl"
   },
   "outputs": [],
   "source": [
    "# @title [1] Setup: Install Libraries\n",
    "\n",
    "!pip install openai pandas scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2179,
     "status": "ok",
     "timestamp": 1753260749855,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "04saDhgtcK2w",
    "outputId": "100360fe-89da-4f73-f60b-0680e6bc0e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# @title [2] Configuration: API Key, Model, and File Paths\n",
    "# In this cell, we configure all the essential parameters for our experiment.\n",
    "# You must replace \"<YOUR_OPENROUTER_API_KEY>\" with your actual key.\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from google.colab import files\n",
    "\n",
    "# --- REQUIRED: SET YOUR API KEY HERE ---\n",
    "# You can get a key from https://openrouter.ai/keys\n",
    "OPENROUTER_API_KEY = \"API-KEY-HERE\"\n",
    "\n",
    "# --- MODEL CONFIGURATION ---\n",
    "# You can change this to any model available on OpenRouter.\n",
    "# Find model names here: https://openrouter.ai/models\n",
    "# Example: \"anthropic/claude-3.5-sonnet\", \"google/gemini-pro-1.5\", \"mistralai/mistral-large\"\n",
    "MODEL_TO_TEST = \"qwen/qwen3-235b-a22b-07-25:free\"\n",
    "\n",
    "# --- DATASET CONFIGURATION ---\n",
    "# Assumes 'PREMOVE.csv' is uploaded to the Colab session's root directory.\n",
    "DATASET_PATH = \"PREMOVE.csv\"\n",
    "\n",
    "# --- EXPERIMENT CONFIGURATION ---\n",
    "# Define the prompting strategies you want to test. The number indicates the number of few-shot examples.\n",
    "SHOT_STRATEGIES = {\n",
    "    \"zero_shot\": 0,\n",
    "    \"one_shot\": 1,\n",
    "    \"two_shot\": 2,\n",
    "    \"five_shot\": 5,\n",
    "    \"ten_shot\": 10\n",
    "}\n",
    "# To run a quick test, you can limit the number of rows processed from the dataset.\n",
    "# Set to None to process all rows.\n",
    "MAX_ROWS_TO_PROCESS = 10 # For demonstration. Set to None for the full run.\n",
    "\n",
    "\n",
    "# --- Initialize the OpenAI Client for OpenRouter ---\n",
    "# We point the base_url to OpenRouter's API endpoint.\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1753260753558,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "6jtH6PXfcYnp",
    "outputId": "c120a075-63c1-462c-848f-c4c7f05acf79"
   },
   "outputs": [],
   "source": [
    "# @title [3] Data Loading and Preparation\n",
    "# This cell loads the PREMOVE.csv file into a pandas DataFrame,\n",
    "# selects the necessary columns, and cleans it up for use.\n",
    "\n",
    "try:\n",
    "    # Load the dataset from the specified path\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "    # Define the columns we need for this task\n",
    "    required_columns = ['VERB TOKEN', 'SENTENCE', 'PREVERB', 'PREVERB SEMANTICS']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    df = df[required_columns]\n",
    "\n",
    "    # Rename columns to be more Python-friendly (no spaces)\n",
    "    df.rename(columns={\n",
    "        'VERB TOKEN': 'verb_token',\n",
    "        'SENTENCE': 'sentence',\n",
    "        'PREVERB': 'preverb',\n",
    "        'PREVERB SEMANTICS': 'ground_truth_semantics'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Drop any rows where essential data might be missing\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Reset the index after dropping rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Limit the number of rows if specified for testing\n",
    "    if MAX_ROWS_TO_PROCESS is not None:\n",
    "        df = df.head(MAX_ROWS_TO_PROCESS)\n",
    "        print(f\"--- Using a limited dataset of {MAX_ROWS_TO_PROCESS} rows for this run. ---\")\n",
    "\n",
    "\n",
    "    print(\"Dataset loaded and prepared successfully.\")\n",
    "    print(f\"Total rows to process: {len(df)}\")\n",
    "    print(\"\\nFirst 5 rows of the prepared data:\")\n",
    "    display(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{DATASET_PATH}' was not found.\")\n",
    "    print(\"Please make sure you have uploaded the CSV file to your Colab environment.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1753260841830,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "Zl2ExuSxcbyh",
    "outputId": "d437007b-4bcf-48ce-86d8-57f2f04e15f1"
   },
   "outputs": [],
   "source": [
    "# @title [4] Core Functions: Prompting and API Interaction\n",
    "# This cell contains the functions that form the backbone of our experiment.\n",
    "\n",
    "def create_prompt(current_row, all_data, num_shots):\n",
    "    '''\n",
    "    Creates a prompt by placing examples in the system message.\n",
    "\n",
    "    Args:\n",
    "        current_row (pd.Series): The row containing the question to ask.\n",
    "        all_data (pd.DataFrame): The entire dataset to sample from for few-shot examples.\n",
    "        num_shots (int): The number of examples to include (0 for zero-shot).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of message dictionaries for the API call.\n",
    "    '''\n",
    "    # --- System Prompt Construction ---\n",
    "    system_prompt_content = (\n",
    "        \"You are an expert in Classical Philology. Your task is to identify the semantic meaning of a given preverb within a sentence from Latin or Ancient Greek. Analyze the verb, sentence, and preverb provided. Respond with ONLY the English meaning and nothing else, using an English adverb or preposition or a multi-word expression.\"\n",
    "    )\n",
    "\n",
    "    if num_shots > 0:\n",
    "        # Exclude the current row from the sampling pool\n",
    "        examples_pool = all_data.drop(current_row.name)\n",
    "        examples = examples_pool.sample(n=num_shots)\n",
    "\n",
    "        example_texts = []\n",
    "        for _, ex in examples.iterrows():\n",
    "            example_texts.append(\n",
    "                f\"Verb: {ex['verb_token']}\\n\"\n",
    "                f\"Sentence: \\\"{ex['sentence']}\\\"\\n\"\n",
    "                f\"Preverb: {ex['preverb']}\\n\"\n",
    "                f\"Meaning: {ex['ground_truth_semantics']}\"\n",
    "            )\n",
    "\n",
    "        system_prompt_content += \"\\n\\nHere are some examples of the task:\\n\\n---\\n\"\n",
    "        system_prompt_content += \"\\n---\\n\".join(example_texts)\n",
    "        system_prompt_content += \"\\n---\\n\\nNow, perform the same analysis for the following case.\"\n",
    "\n",
    "    # --- User Prompt Construction ---\n",
    "    # The user message is now clean, containing only the data to be processed.\n",
    "    user_prompt_content = (\n",
    "        f\"Verb: {current_row['verb_token']}\\n\"\n",
    "        f\"Sentence: \\\"{current_row['sentence']}\\\"\\n\"\n",
    "        f\"Preverb: {current_row['preverb']}\\n\"\n",
    "        f\"Meaning:\"\n",
    "    )\n",
    "\n",
    "    # --- Final Message Assembly ---\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_content}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_llm_response(messages, model):\n",
    "    \"\"\"\n",
    "    Sends a request to the OpenRouter API and returns the model's response.\n",
    "\n",
    "    Args:\n",
    "        messages (list): The list of message dictionaries for the prompt.\n",
    "        model (str): The name of the model to query.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the model's response, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.3, # Set to 0 for deterministic, factual outputs\n",
    "            max_tokens=50,   # The answer should be short\n",
    "        )\n",
    "        response_text = completion.choices[0].message.content\n",
    "        # Clean the output: remove extra whitespace and quotes\n",
    "        return response_text.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"  !! API Error: {e}\")\n",
    "        return \"API_ERROR\"\n",
    "\n",
    "print(\"Core functions updated and defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97652,
     "status": "ok",
     "timestamp": 1753260945276,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "x3daeayYdUqZ",
    "outputId": "5af11105-9aa4-4142-ff5a-1a45c417ee2f"
   },
   "outputs": [],
   "source": [
    "# @title [5] Evaluation Engine: Run Experiments\n",
    "# This is the main part of the notebook. It iterates through each prompting strategy,\n",
    "# queries the model for every row, and evaluates the results.\n",
    "# The results now include the 'verb_token' for more detailed analysis.\n",
    "\n",
    "all_results_dfs = {}\n",
    "\n",
    "for strategy_name, num_shots in SHOT_STRATEGIES.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ðŸš€ Starting evaluation for: {strategy_name.upper()} ({num_shots}-shot)\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"  -> Processing row {index + 1}/{len(df)}...\")\n",
    "\n",
    "        # 1. Create the prompt using our unified function\n",
    "        if num_shots > 0 and len(df) <= num_shots:\n",
    "            print(f\"  !! Warning: Not enough data ({len(df)} rows) to create a {num_shots}-shot prompt. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        prompt_messages = create_prompt(row, df, num_shots)\n",
    "\n",
    "        # 2. Get the LLM's prediction\n",
    "        prediction = get_llm_response(prompt_messages, MODEL_TO_TEST)\n",
    "\n",
    "        # Add a small delay to respect rate limits, if any\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 3. Normalize ground truth and prediction for accurate comparison\n",
    "        ground_truth = str(row['ground_truth_semantics']).lower().strip()\n",
    "        prediction_normalized = prediction.lower().strip()\n",
    "\n",
    "        # 4. Check if the prediction is correct\n",
    "        is_correct = (ground_truth == prediction_normalized)\n",
    "\n",
    "        # 5. Store the results (now including the verb_token)\n",
    "        results_data.append({\n",
    "            'verb_token': row['verb_token'],\n",
    "            'sentence': row['sentence'],\n",
    "            'preverb': row['preverb'],\n",
    "            'ground_truth': row['ground_truth_semantics'],\n",
    "            'llm_prediction': prediction,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "\n",
    "    # --- Analysis and Reporting for the current strategy ---\n",
    "    if not results_data:\n",
    "        print(\"\\nNo results were generated for this strategy.\")\n",
    "        continue\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    all_results_dfs[strategy_name] = results_df\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = results_df['is_correct'].mean()\n",
    "\n",
    "    print(f\"\\n--- ðŸ“Š Results for {strategy_name.upper()} ---\")\n",
    "    print(f\"Model Tested: {MODEL_TO_TEST}\")\n",
    "    print(f\"Total Items Evaluated: {len(results_df)}\")\n",
    "    print(f\"Correct Predictions: {results_df['is_correct'].sum()}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(\"------------------------------------------\\n\")\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    output_filename = f\"results_{MODEL_TO_TEST.replace('/', '_')}_{strategy_name}.csv\"\n",
    "    results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"âœ… Detailed results saved to '{output_filename}'\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All evaluations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1753260945348,
     "user": {
      "displayName": "Michele Ciletti",
      "userId": "16387247527285619265"
     },
     "user_tz": -120
    },
    "id": "riU2rgOddjiv",
    "outputId": "820e98fb-a027-4f2e-c562-7a002999cdf4"
   },
   "outputs": [],
   "source": [
    "# @title [6] Download Result Files\n",
    "# This cell provides the code to download the generated CSV files to your local machine.\n",
    "\n",
    "if all_results_dfs:\n",
    "    print(\"Preparing result files for download...\")\n",
    "    for strategy_name in all_results_dfs.keys():\n",
    "        filename = f\"results_{MODEL_TO_TEST.replace('/', '_')}_{strategy_name}.csv\"\n",
    "        print(f\"  - Downloading {filename}...\")\n",
    "        files.download(filename)\n",
    "else:\n",
    "    print(\"No result files were generated to download.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+siuw4003JS8g+0QUAN/Y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
